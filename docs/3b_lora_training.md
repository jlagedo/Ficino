# LoRA adapter training for Apple's on-device foundation model

Apple's ~3B-parameter on-device foundation model (AFMTextV7) ships as a heavily quantized 2-bit base that relies on **LoRA adapters** to both recover lost quality and specialize for specific tasks — and as of WWDC 2025, Apple now lets third-party developers train and deploy their own rank-32 LoRA adapters through a Python-based toolkit and the Foundation Models framework. Custom adapters unlock domain expertise, output-style control, and classification accuracy that prompt engineering alone cannot reliably achieve, at roughly **160 MB per adapter** with **66.6 million trainable parameters** (2.1% of the model). This document covers every layer of the system: model architecture, LoRA configuration, the training toolkit, Swift integration, distribution, versioning, and hard-won practical lessons from early adopters.

---

## The base model: a two-segment 3B-parameter transformer

The 2025 on-device model (AFMTextV7) is a dense decoder-only transformer with a distinctive **two-segment hybrid** architecture designed to minimize memory through KV-cache sharing.

**Core specifications:**

| Property | Value |
|---|---|
| Total parameters | **3,178,001,792 (~3.18B)** |
| Architecture | Two-segment transformer decoder |
| Segment 0 (full attention) | 35 layers |
| Segment 1 (KV-reuse attention) | 21 layers |
| Hidden dimension | 2,048 |
| Attention heads | 16 (grouped-query, 8 KV heads) |
| Head dimension | 128 |
| Feed-forward dimension | 6,656 (3.25× expansion, SwiGLU) |
| Vocabulary | 153,600 tokens |
| Context window | 4,096 tokens |
| Positional encoding | RoPE (θ = 500,000) |
| Normalization | RMSNorm (pre-attention, pre-FFN, final) |
| Quantization | 2-bit QAT (weights), 4-bit (embeddings), 8-bit (KV cache) |
| Production memory | ~1.0–1.1 GB |
| Inference speed | 0.6 ms TTFT per prompt token; 30 tok/s on iPhone 15 Pro |

Segment 0's 35 layers compute full query, key, and value projections. Segment 1's 21 layers compute **only queries** and reuse keys and values from Segment 0, yielding a **37.5% reduction** in KV-cache memory and prefill latency. The model uses shared input/output embeddings and QK normalization via RMSNorm. In the 2025 update, the vocabulary expanded from the original 49K to 153,600 tokens, a vision encoder (ViTDet-L, 300M parameters) was added, and the training pipeline introduced a novel dense→MoE upcycle→distillation approach where a 64-expert MoE teacher trained on 1T tokens was distilled back into the dense student.

The 2-bit quantization uses a balanced set **{−1.5, −0.5, 0.5, 1.5}** with a learnable scaling factor per weight tensor, trained end-to-end via Quantization-Aware Training with straight-through estimators. This aggressive compression is what makes the adapter system essential rather than optional.

---

## Apple's two-tier adapter architecture explains everything

The LoRA adapter system operates on two tiers, and understanding this hierarchy is critical for developers.

**Tier 1: Accuracy-recovery adapters** compensate for quality lost during 2-bit quantization. These are trained on the same pre-training and post-training data as the base model — roughly **10 billion tokens** (just 0.15% of the full training corpus). The quantized-plus-adapter combination recovers most of the unquantized model's performance: at 3.7 bits-per-weight, the 2024 model recovered from 87.3% to **94.8%** of unquantized AlpacaEval performance, and from 91.3% to **96.0%** on GSM8K. The more aggressive the quantization, the more quality the adapters recover. These adapters are available in ranks {8, 16, 32} internally.

**Tier 2: Task-specific (feature) adapters** are initialized **from the accuracy-recovery adapter weights** — not randomly. This initialization means each feature adapter inherits quantization-error correction for free. The adapters are then fine-tuned on task-specific data (often synthetic data generated by Apple's larger server model, filtered via rejection sampling). Each Apple Intelligence feature — summarization, Smart Reply, content tagging, writing tools, notification prioritization, Visual Intelligence — gets its own adapter that is dynamically loaded, cached in memory, and **hot-swapped at runtime** against the single frozen base model.

Apple's internal adapters use **rank 16** as the optimal balance of capacity and inference cost, with adapter sizes in the **tens of megabytes**. The developer-facing toolkit uses **rank 32**, producing larger ~160 MB adapters, presumably to give third parties more capacity for diverse custom tasks.

For the server model, Apple uses a related but distinct technique: before ASTC compression to 3.56 bits-per-weight, the most significant singular vectors of adapted layers are pulled out into the LoRA adapter (similar to PiSSA), and only the residuals are compressed, reducing compression error.

---

## LoRA configuration: rank 32 across attention and feed-forward layers

The developer toolkit's LoRA configuration applies low-rank adaptation to **every linear projection** in both the attention and feed-forward blocks across all 56 layers.

| LoRA Parameter | Value |
|---|---|
| Rank (r) | **32** |
| Alpha (α) | **16** |
| Scaling factor (α/r) | **0.5** |
| Dropout | **0.0** |
| Parameter precision | 16-bit |
| Total trainable parameters | **66,633,728** (2.1% of model) |
| Frozen parameters | 3,111,368,064 (97.9%) |
| Total LoRA modules | 350 computation units |
| Exported adapter size | **~160 MB** |

**In Segment 0 (35 layers), each layer has LoRA on:**
- **QKV combined projection** (LoRAFusedMultiOutputLinear): 278,528 parameters — covering query (2048→2048), key (2048→256), and value (2048→256) projections with separate lora_0, lora_1, lora_2 sub-modules
- **Attention output projection**: 131,072 parameters
- **FFN linear_0** (input→hidden): 278,528 parameters
- **FFN linear_1** (gate projection): 278,528 parameters
- **FFN output** (hidden→output): 278,528 parameters
- **Per-layer total**: 1,245,184 trainable parameters
- **Segment 0 total**: 43,581,440

**In Segment 1 (21 layers), each layer has LoRA on:**
- **Query transform** only (no K/V since they're reused): 131,072 parameters
- **Attention output projection**: 131,072 parameters
- **FFN linear_0, linear_1, output**: 278,528 each (835,584 total)
- **Per-layer total**: 1,097,728 trainable parameters
- **Segment 1 total**: 23,052,288

The LoRA modules are managed through 280 `AdaptedLayer` wrappers and 280 `ModuleDict` containers, with 1,173 total individual components. The scaling factor of 0.5 means the adapter's contribution to each layer is halved relative to rank — a conservative choice that reduces the risk of catastrophic drift from the base model's behavior while still allowing meaningful specialization. Zero dropout reflects Apple's assumption that developer training datasets will be relatively small and carefully curated.

---

## The training toolkit: from JSONL to .fmadapter

### Setup requirements

The Python-based adapter training toolkit (current version: **26.0.0**, the first non-beta release) requires:

- **Hardware**: Mac with Apple Silicon and **at least 32 GB unified memory**, or a Linux machine with a capable GPU. In practice, 16–18 GB Macs consistently hit MPS out-of-memory errors (one developer reported a 22.64 GB allocation attempt on an 18 GB M3 Pro). Cloud training on **NVIDIA A100 or H100** GPUs works well — one developer trained ~400 examples in about one hour on a single H100.
- **Software**: Python 3.11+, conda recommended. The toolkit's `requirements.txt` handles dependencies.
- **Entitlement**: The **Foundation Models Framework Adapter Entitlement** (`com.apple.developer.foundation-model-adapter`) is required **only for deployment** in shipping apps, not for training or local testing. The Account Holder of an Apple Developer Program membership must request it. Approved developers receive email notifications when new toolkit versions ship.

```bash
conda create -n adapter-training python=3.11
conda activate adapter-training
cd /path/to/adapter_training_toolkit
pip install -r requirements.txt
```

The toolkit ships with a Jupyter notebook (`examples/end_to_end_example.ipynb`) that walks through the complete pipeline, plus base model assets in the `assets/` directory for local inference verification.

### Training data format

Training data is JSONL where **each line is a JSON array** of message objects:

```jsonl
[{"role": "user", "content": "Summarize this email: ..."}, {"role": "assistant", "content": "The email discusses ..."}]
```

With an optional system message:

```jsonl
[{"role": "system", "content": "You are a specialist in medical terminology."}, {"role": "user", "content": "Define 'dyspnea'."}, {"role": "assistant", "content": "Dyspnea is the medical term for shortness of breath ..."}]
```

The `role` field accepts `"user"`, `"assistant"`, and `"system"`. As of toolkit version 0.2.0, the schema also supports **tool-calling data** and **guided generation** structures (documented in `Schema.md` inside the toolkit's `docs/` directory). Version 26.0.0 added support for custom data transforms in the training pipeline.

Apple recommends **100–1,000 samples** for basic tasks and **5,000+ samples** for complex tasks, emphasizing quality over quantity. A practical example: one developer achieved meaningful behavioral modification with just ~400 GPT-4o-generated prompt/response pairs, though they needed 100 training epochs to see strong effects. Apple's own internal adapters use synthetic data generated by the larger server model and filtered through rejection sampling and rule-based validators (length, formatting, point-of-view, entailment).

A critical practical insight from early adopters: **Apple's model is anchored to short responses**. Training data with long assistant responses will often be truncated during inference. Place the most important behavioral modifications at the **beginning** of response examples, not the end.

### Training, evaluation, and generation

```bash
# Train the adapter
python -m examples.train_adapter \
  --train-data /path/to/train.jsonl \
  --eval-data /path/to/valid.jsonl \
  --epochs 5 \
  --learning-rate 1e-3 \
  --batch-size 4 \
  --checkpoint-dir /path/to/checkpoints/

# Optionally train a draft model for speculative decoding
python -m examples.train_draft_model \
  --checkpoint /path/to/checkpoints/adapter-final.pt \
  --train-data /path/to/train.jsonl \
  --eval-data /path/to/valid.jsonl \
  --epochs 5 \
  --learning-rate 1e-3 \
  --batch-size 4 \
  --checkpoint-dir /path/to/checkpoints/

# Test generation
python -m examples.generate \
  --prompt "Your test prompt" \
  --checkpoint /path/to/checkpoints/adapter-final.pt \
  --draft-checkpoint /path/to/checkpoints/draft-model-final.pt
```

The Python API also exposes an `AdapterTrainingConfiguration` class with additional parameters:

```python
AdapterTrainingConfiguration(
    epochs=6,
    learning_rate=1e-4,
    batch_size=2,
    gradient_accumulation_steps=4,
    enable_activation_checkpointing=True,
    precision='bf16-mixed',
    max_sequence_length=4095,
    compile_model=False
)
```

One caution: `bf16` precision training has been reported to produce degenerate outputs ("Vac Vac Vac...") in some configurations. Full-precision training on L4 GPUs can also OOM; **A100 or H100 GPUs are the reliable choices** for cloud training.

The optional draft model (a ~48.8M parameter, 12-layer mini-transformer sharing the 153,600-token vocabulary) enables **speculative decoding** at inference time — generating 4–8 candidate tokens simultaneously with 60–80% acceptance rate, yielding a 2–4× inference speedup.

### Packaging as .fmadapter

```bash
python -m export.export_fmadapter \
  --adapter-name my_adapter \
  --checkpoint /path/to/checkpoints/adapter-final.pt \
  --draft-checkpoint /path/to/checkpoints/draft-model-final.pt \
  --output-dir /path/to/exports/
```

**Do not modify anything in the `export/` directory** — the export logic must match the runtime loader exactly. The `.fmadapter` output is a package directory containing adapter weight files and a `metadata.json`:

```json
{
    "adapterIdentifier": "fmadapter-my_adapter-9799725",
    "author": "Your Name",
    "baseModelSignature": "9799725ff8e851184037110b422d891ad3b92ec1",
    "creatorDefined": {},
    "description": "Custom domain adapter.",
    "license": "",
    "loraRank": 32,
    "speculativeDecodingDraftTokenCount": 5
}
```

The `adapterIdentifier` must match the regex `/fmadapter-\w+-\w+/` — **hyphens in the adapter name portion will cause silent load failures** (`compatibleAdapterNotFound` error). Use underscores only. The `baseModelSignature` is a SHA hash hardcoded in `export/constants.py` that locks each adapter to a **single specific system model version**. If the OS's model version doesn't match, the adapter will not load.

---

## Swift integration: loading and using adapters at runtime

### Loading a custom adapter

```swift
import FoundationModels

// Load from a file URL (e.g., downloaded via Background Assets)
let adapterURL = URL(filePath: "/path/to/my_adapter.fmadapter")
let adapter = try SystemLanguageModel.Adapter(fileURL: adapterURL)
let adaptedModel = SystemLanguageModel(adapter: adapter)
let session = LanguageModelSession(model: adaptedModel)

let response = try await session.respond(to: "Your prompt here")
print(response.content)
```

```swift
// Load from the app bundle (for testing; not recommended for production)
if let url = Bundle.main.url(forResource: "my_adapter", withExtension: "fmadapter") {
    let adapter = try SystemLanguageModel.Adapter(fileURL: url)
    let model = SystemLanguageModel(adapter: adapter)
    let session = LanguageModelSession(model: model)
}
```

```swift
// Name-based initializer (for registered adapters)
let adapter = try await SystemLanguageModel.Adapter(name: "my_adapter")
```

Sessions can include instructions, tools, and guardrails alongside adapters:

```swift
let session = LanguageModelSession(
    model: adaptedModel,
    guardrails: .default,
    tools: [MyCustomTool()],
    instructions: "You are a domain-specific assistant."
)
```

Apple's built-in adapters are accessible via use-case identifiers — for example, the content-tagging adapter powers entity extraction and topic classification:

```swift
let taggingModel = SystemLanguageModel(useCase: .contentTagging)
let session = LanguageModelSession(model: taggingModel,
    instructions: "Tag the 3 most important actions and emotions in the input.")

@Generable struct Tags {
    @Guide(.maximumCount(3)) let actions: [String]
    @Guide(.maximumCount(3)) let emotions: [String]
}
let result = try await session.respond(to: inputText, generating: Tags.self)
```

### Distribution via Background Assets

At ~160 MB per adapter, bundles should **not** include adapters in the main app binary. Apple recommends the **Background Assets framework**, which now has a dedicated asset pack type for Foundation Models adapters. Adapters are hosted on your own server or Apple's servers, and each user downloads only the single adapter compatible with their device's current OS version. This is especially important because you will need to maintain **multiple adapter versions** simultaneously as users run different OS versions.

---

## When adapters beat prompt engineering, and when they don't

Adapters are an **advanced technique** with significant ongoing maintenance costs. Apple explicitly recommends exhausting prompt engineering and tool calling before reaching for adapter training.

**Adapters are justified when:**
- The model needs genuine domain expertise (medical terminology, legal language, brand-specific product catalogs) that cannot be injected through prompt context
- Consistent adherence to a specific output style, format, or policy is required across thousands of interactions
- Prompt engineering hits a ceiling on accuracy or consistency despite careful iteration
- Latency matters — an adapter eliminates the token overhead of lengthy few-shot examples in the prompt, freeing context-window capacity and reducing time-to-first-token
- You need to replicate server-based fine-tuned behavior fully on-device

**Prompt engineering is better when:**
- The task is within the base model's general capabilities (summarization, simple classification, extraction)
- Requirements change frequently — prompt updates are instant; adapter retraining takes hours
- You want to avoid the mandatory retraining cycle when Apple updates the base model with OS releases
- You're prototyping or validating whether the on-device model is suitable at all

The **context window is only 4,096 tokens** and must be shared between input and output. Adapters help here by encoding learned behavior in weights rather than consuming tokens with few-shot examples. However, one developer reported a bug where loading a custom adapter caused a single prompt to consume **90% of the context window** (versus 1% without the adapter), likely due to training configuration issues — a cautionary tale about testing context utilization carefully.

---

## Versioning, retraining, and the maintenance burden

The single most consequential operational detail about Apple's adapter system is version-locking. **Each adapter works with exactly one system model version.** When Apple updates the base model as part of an OS release, every custom adapter must be retrained with the new toolkit version, re-exported, and redistributed.

Apple releases a new toolkit for every system model update. The system model is shared across iOS, macOS, iPadOS, and visionOS, and not every OS point release changes the model. Developers with the adapter entitlement receive email notifications when new toolkits are available. Apple advises installing beta software early to allow time for retraining before public release.

There is **no API to query which model version a user's device is running**. Apple has acknowledged this gap and suggests filing Feedback requests. The practical workaround is to host adapters on a server and use the Background Assets framework to deliver the correct version based on the user's OS version.

This retraining requirement is non-trivial. The toolkit has already gone through at least three versions (Beta 0.1.0, Beta 0.2.0, and 26.0.0) with breaking changes — earlier beta toolkit versions were removed from distribution. Each version carries a different `BASE_SIGNATURE` in `export/constants.py`. Developers should maintain automated training pipelines and golden evaluation sets that can validate adapter quality across toolkit versions.

**Practical guidance on overfitting**: with small datasets (100–400 examples), early adopters found that the default 2 epochs often produce minimal behavioral change, while 100 epochs forced strong adaptation at the cost of potential overfitting to training patterns. Apple recommends 3–5 epochs as a reasonable starting range. Use the eval split to monitor loss divergence. The model's short-response bias acts as a natural regularizer against some forms of overfitting but can also mask insufficient training if evaluation only checks short outputs.

---

## How Apple's own adapters work internally

Apple's internal adapters illustrate the system at its most sophisticated. The **summarization adapter** — covering emails, messages, and notifications — was trained on synthetic summaries generated by the larger AFM-server model. The data pipeline includes:

- Input payloads from public datasets, vendor data, and internally generated examples, all anonymized
- Rule-based filters (length, formatting, point-of-view, voice consistency)
- Model-based entailment filters ensuring summaries are faithful to source
- Explicit prompt-injection mitigation: cases where the model followed instructions embedded in the email content rather than summarizing were identified, correct summaries were generated by the server model, and these were added to the training mixture

Evaluation used **750 carefully sampled responses** per use case, scored on composition, comprehensiveness, groundedness, instruction-following, and harmfulness. The on-device adapter outperformed Phi-3-mini, Llama-3-8B, and Gemma-7B on summarization quality despite the base model being smaller.

Other known internal adapters include **Smart Reply** (generating contextual message responses), **writing tools** (rewriting, tone adjustment, proofreading), **notification prioritization**, **Visual Intelligence** (creating calendar events from images of flyers, etc.), **in-app actions**, and **multilingual safety alignment** adapters. Each is hot-swapped against the single base model at runtime, with the OS managing memory to ensure system responsiveness.

---

## Conclusion

Apple's LoRA adapter system is architecturally elegant — a two-tier design where accuracy-recovery adapters compensate for extreme 2-bit quantization while task-specific adapters inherit that correction and add specialization. The rank-32 developer configuration with α=16, scaling 0.5, and zero dropout across all 56 layers' attention and feed-forward projections provides **66.6 million trainable parameters** — enough capacity for meaningful domain adaptation while keeping adapters at a manageable 160 MB.

The key practical realities are: training requires **32 GB+ Apple Silicon or cloud A100/H100 GPUs**; the model is biased toward short responses, which must be reflected in training data design; each adapter is locked to a single OS model version requiring ongoing retraining; and the 4,096-token context window makes adapter-based learning preferable to token-heavy few-shot prompting for production use. The tooling is functional but young — expect naming bugs, incomplete documentation around tool-calling training data, and evolving export formats. Developers who invest in automated retraining pipelines and robust evaluation sets will be best positioned to maintain adapters across Apple's model evolution cycle.